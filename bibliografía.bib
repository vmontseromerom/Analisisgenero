@Article{tidyverse2019,
  title = {Welcome to the {tidyverse}},
  author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686
    }
    
@Article{@REAT2018,
    title = {{REAT}: A {R}egional {E}conomic {A}nalysis {T}oolbox for {R}},
    author = {Thomas Wieland},
    year = {2019},
    journal = {REGION},
    volume = {6},
    number = {3},
    pages = {R1--R57},
    url = {https://doi.org/10.18335/region.v6i3.267},
  }
@article{Croarkin2014,
   abstract = {The NIST/SEMATECH e-Handbook of Statistical Methods1, is a Web-based book whose goal is to help scientists and engineers incorporate statistical methods into their work as efficiently as possible. Ideally it will serve as a reference that will help scientists and engineers design their own experiments and carry out the appropriate analyses when a statistician is not available to help. It is also hoped that it will serve as a useful educational tool that will help users of statistical methods and consumers of statistical information better understand statistical procedures and their underlying assumptions and more clearly interpret scientific and engineering results stated in statistical terms.},
   author = {C Croarkin and P Tobias},
   journal = {Retrieved January},
   title = {NIST/SEMATECH e-handbook of statistical methods},
   volume = {1},
   year = {2014},
}

  @Manual{RSoftware,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2022},
    url = {https://www.R-project.org/},
  }
  
  @generic{OpenScience2011,
   abstract = {Background: There is increasing interest to make primary data from published research publicly available. We aimed to assess the current status of making research data available in highly-cited journals across the scientific literature. Methods and Results: We reviewed the first 10 original research papers of 2009 published in the 50 original research journals with the highest impact factor. For each journal we documented the policies related to public availability and sharing of data. Of the 50 journals, 44 (88%) had a statement in their instructions to authors related to public availability and sharing of data. However, there was wide variation in journal requirements, ranging from requiring the sharing of all primary data related to the research to just including a statement in the published manuscript that data can be available on request. Of the 500 assessed papers, 149 (30%) were not subject to any data availability policy. Of the remaining 351 papers that were covered by some data availability policy, 208 papers (59%) did not fully adhere to the data availability instructions of the journals they were published in, most commonly (73%) by not publicly depositing microarray data. The other 143 papers that adhered to the data availability instructions did so by publicly depositing only the specific data type as required, making a statement of willingness to share, or actually sharing all the primary data. Overall, only 47 papers (9%) deposited full primary raw data online. None of the 149 papers not subject to data availability policies made their full primary data publicly available. Conclusion: A substantial proportion of original research papers published in high-impact journals are either not subject to any data availability policies, or do not adhere to the data availability instructions in their respective journals. This empiric evaluation highlights opportunities for improvement. © 2011 Alsheikh-Ali et al.},
   author = {Alawi A. Alsheikh-Ali and Waqas Qureshi and Mouaz H. Al-Mallah and John P.A. Ioannidis},
   doi = {10.1371/journal.pone.0024357},
   issn = {19326203},
   issue = {9},
   journal = {PLoS ONE},
   title = {Public availability of published research data in High-Impact journals},
   volume = {6},
   year = {2011},
}

@generic{OpenScience2013,
   author = {Geir Kjetil Sandve and Anton Nekrutenko and James Taylor and Eivind Hovig},
   doi = {10.1371/journal.pcbi.1003285},
   issn = {1553734X},
   issue = {10},
   journal = {PLoS Computational Biology},
   title = {Ten Simple Rules for Reproducible Computational Research},
   volume = {9},
   year = {2013},
}
